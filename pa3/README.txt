PS3
John Doherty (doherty1)

The map-reduce setup that I use for this assignment is as follows. I parse the input files into a set of key-value pairs where the key is the title of the wikipedia article and the value is the text of the article. Each key-value pair is passed into a mapper that extracts the set of n-grams from the article text. It then checks whether each n-gram is in a HashSet of n-grams obtained from the query article. The total number of matches is the similarity score. Each mapper's key output is the same and the value output contains the score and article title. The reducer then finds the article with the highest similarity score.

This algorithm uses just one map-reduce pass with the mapping computing the scores and the reduce computing the max. Creating the initial HashSet of n-grams for the query document is linear in the size of the query document. We call that O(q). Each mapping is performing an operation that is linear in the size of the article it is attempting to match to the query article. This is because the comparison of each n-gram takes constant time because it is just a hash lookup. The entire mapping operation is then linear in the size of the input corpus. Since we split this mapping up amoung P processors, the time complexity of the mapping is O(n/P) where n is the size of the input corpus. Finally, the reducer receives one value from each mapper and is performing a max over these values. We can say that this reduction is roughly over O(P) values and thus could theoretically be performed in O(log(P)). Summing this all up gives us a total time complexity of O(q + P + log(P)).

